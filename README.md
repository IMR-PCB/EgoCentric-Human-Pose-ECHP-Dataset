# EgoCentric-Human-Action(ECHA) Dataset
This is the official version of ECHA dataset in the paper **"EgoFish3D: Egocentric 3D Pose Estimation from a Fisheye Camera via Self-Supervised Learning"**
![overview](https://user-images.githubusercontent.com/86871168/147398404-7ee8fcad-24a8-4a7a-89ad-5288c7bfccdd.png)

Information:
- [x] Images from the third-person view and the egocentric view for a self-supervised learning.
- [x] Diverse real-world scenarios.
- [x] Different subjects performing many daily actions in different body textures.

## Introduction
To address the challenges in egocentric 3D pose estimation, we propose a real-world egocentric human action dataset, named ECHA, using a head-mounted GoPro camera with a fisheye lens. The training and validation sets of the ECHA dataset consist of **30** video sequences(**fps=30Hz**) recorded in **8** diverse real-world indoor/outdoor scenes with **10** different daily actions performed by **9** subjects in **20** different body textures. The ten daily actions include: squatting, walking, dancing, stretching, waving, boxing, kicking, touching, clamping, knocking. The test set of the ECHA dataset consists of **7** video sequences by 4 subjects with new body textures captured by a multi-camera motion capture system with ground truth annotations.

![indoor_3](https://user-images.githubusercontent.com/86871168/147669662-4c266356-ef4d-46a2-ad81-90e3c43b21cc.png)

![placement](https://user-images.githubusercontent.com/86871168/147398382-bab0790b-7e32-475b-a412-4f4d2486c69b.png)

## Overview
![actions](https://user-images.githubusercontent.com/86871168/147398391-418eebfc-05eb-4a70-a78c-444ddfe7f2a5.png)

## Citation
```
  cite here:
```
