# EgoCentric-Human-Action(ECHA) Dataset
This is the official version of ECHA dataset
## Introduction
To address the challenges in egocentric 3D pose estimation, we propose a real-world egocentric human action dataset, named ECHA, using a head-mounted GoPro camera with a fisheye lens. The training and validation sets of the ECHA dataset consist of **30** video sequences(**fps=30Hz**) recorded in **8** diverse real-world indoor/outdoor scenes with **10** different daily actions performed by **9** subjects in **20** different body textures. The ten daily actions include: squatting, walking, dancing, stretching, waving, boxing, kicking, touching, clamping, knocking. The test set of the ECHA dataset consists of **7** video sequences by 4 subjects with new body textures captured by a multi-camera motion capture system with ground truth annotations.


![placement](https://user-images.githubusercontent.com/86871168/147398382-bab0790b-7e32-475b-a412-4f4d2486c69b.png)

## Overview
![actions](https://user-images.githubusercontent.com/86871168/147398391-418eebfc-05eb-4a70-a78c-444ddfe7f2a5.png)
![overview](https://user-images.githubusercontent.com/86871168/147398404-7ee8fcad-24a8-4a7a-89ad-5288c7bfccdd.png)

## Citation
```
  cite here:
```
